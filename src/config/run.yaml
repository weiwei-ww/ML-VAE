# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 123456
__set_seed: !apply:torch.manual_seed [!ref <seed>]

dataset: !PLACEHOLDER
model_class: !PLACEHOLDER
model_name: !PLACEHOLDER

output_dir: !ref results/<model_name>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <output_dir>/train_log.txt

# Data files
n_phonemes: 39
prepare:
    dataset_dir: !ref datasets/<dataset>/original_dataset  # e.g. /path/to/dataset
    train_json_path: !ref datasets/<dataset>/annotation/train.json
    valid_json_path: !ref datasets/<dataset>/annotation/valid.json
    test_json_path: !ref datasets/<dataset>/annotation/test.json
    phoneme_set_handler: !new:utils.phonemes.PhonemeSetHandler
        n_phonemes: !ref <n_phonemes>


blank_index: 0
unk_index: 1



# Feature parameters
sample_rate: 16000
hop_length: 10
n_fft: 400
n_mels: 40



augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment
    sample_rate: !ref <sample_rate>
    speeds: [95, 100, 105]

compute_features: !new:speechbrain.lobes.features.Fbank
    sample_rate: !ref <sample_rate>
    hop_length: !ref <hop_length>
    n_fft: !ref <n_fft>
    n_mels: !ref <n_mels>




# Training parameters
batch_size: 8

# Dataset and dataloader options
sorting: descending # choose between ascending, descending and random
train_dataloader_opts:
    batch_size: !ref <batch_size>
valid_dataloader_opts:
    batch_size: !ref <batch_size>
test_dataloader_opts:
    batch_size: !ref <batch_size>

model: !PLACEHOLDER
    output_dir: !ref <output_dir>
    train_logger: !ref <train_logger>
    input_size: !ref <n_mels>
    n_phonemes: !ref <n_phonemes>

